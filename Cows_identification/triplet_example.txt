import torch
import torch.nn as nn
from torchvision import models, transforms
from torch.optim import Adam
from torch.utils.data import DataLoader
from my_triplet_dataset import MyTripletDataset  # This should be your custom dataset

# Parameters
learning_rate = 0.001
num_epochs = 10
batch_size = 32
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Create the model
model = models.resnet50(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, 128)  # We change the last layer to output embeddings of size 128
model = model.to(device)

# Triplet loss
criterion = nn.TripletMarginLoss(margin=1.0, p=2)

# Optimizer
optimizer = Adam(model.parameters(), lr=learning_rate)

# Prepare the data loaders
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

train_dataset = MyTripletDataset('path/to/train/data', transform=transform)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Training loop
for epoch in range(num_epochs):
    for i, (anchor, positive, negative) in enumerate(train_loader):
        # Move to device
        anchor = anchor.to(device)
        positive = positive.to(device)
        negative = negative.to(device)

        # Forward pass
        anchor_embedding = model(anchor)
        positive_embedding = model(positive)
        negative_embedding = model(negative)

        # Compute the loss
        loss = criterion(anchor_embedding, positive_embedding, negative_embedding)

        # Backward pass and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if (i + 1) % 100 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item()}')
